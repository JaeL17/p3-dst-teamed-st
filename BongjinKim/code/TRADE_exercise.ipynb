{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b776bae3-9ccd-4bb2-b853-0da959b081a0",
   "metadata": {},
   "source": [
    "# TRADE 실습\n",
    "😁 본 노트북은 TRADE를 한 줄 씩 실행하며 Input과 Output의 전체적인 Flow를 학습하기 위한 것입니다. 이를 통해 DST TRADE Model Task에 대해 올바른 이해를 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f46572-1655-4351-84aa-5242e259c963",
   "metadata": {},
   "source": [
    "### 1. TRADE_preprocessor\n",
    "convert examples to features (input_ids, segment_ids, target_ids, slot_meta) 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f7adc881-9b8e-467d-8cdd-726a23197f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n",
    "from data_utils import *\n",
    "from preprocessor import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20168b5a-192e-4292-aa24-f5797ac8313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/code\n"
     ]
    }
   ],
   "source": [
    "cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "619bf92a-6480-4e85-b224-dfa53242d5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3671.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word drop: 0.0\n",
      "현재 2 개의 주제에 대해 각각 턴으로 추출한 총 대화데이터셋 개수 : 12 개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#-> json 파일 불러와서 DSTInputExample 객체 생성\n",
    "train_data, dev_data, dev_label = load_dataset(\"/opt/ml/input/data/train_dataset/train_dials.json\")\n",
    "with open(\"/opt/ml/input/data/train_dataset/slot_meta.json\") as f:\n",
    "    slot_meta = json.load(f)\n",
    "with open(\"/opt/ml/input/data/train_dataset/ontology.json\") as f:\n",
    "    ontology = json.load(f)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dsksd/bert-ko-small-minimal\")\n",
    "    \n",
    "#대화 2개 추출, 단 턴의 길이는 각자 다름\n",
    "train_data = train_data[0:2]\n",
    "\n",
    "#대화를 턴 별로 DSTInputExample 객체에 맞게 추출\n",
    "train_example = get_examples_from_dialogues(train_data)\n",
    "\n",
    "#tokenizing\n",
    "processor = TRADEPreprocessor(slot_meta, tokenizer)\n",
    "train_dataset = processor.convert_examples_to_features(train_example)\n",
    "print(f\"현재 {len(train_data)} 개의 주제에 대해 각각 턴으로 추출한 총 대화데이터셋 개수 : {len(train_dataset)} 개\")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4bf4eda2-0ae1-468a-be30-9352064ba92d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 178.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 12 개의 features가 있습니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[OpenVocabDSTFeature(guid='snowy-hat-8324:관광_식당_11-0', input_id=[2, 3, 6265, 6672, 4073, 3249, 4034, 8732, 4292, 6722, 4076, 8553, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [8732, 3, 0], [21832, 11764, 3], [6265, 6672, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='snowy-hat-8324:관광_식당_11-1', input_id=[2, 3, 6265, 6672, 4073, 3249, 4034, 8732, 4292, 6722, 4076, 8553, 3, 11655, 4279, 8553, 18, 6336, 4481, 22014, 6771, 4204, 4112, 8538, 4147, 27233, 35, 18790, 4086, 24, 4469, 10749, 14043, 4006, 4073, 4325, 3311, 4112, 6392, 4110, 2734, 4219, 3249, 4576, 6216, 18, 3, 3311, 4116, 4150, 7149, 18790, 4112, 2633, 4151, 4076, 5240, 4050, 6698, 4467, 4029, 4070, 13177, 4479, 4065, 4150, 35, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6336, 4481, 22014, 6771, 4204, 3], [8732, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='snowy-hat-8324:관광_식당_11-2', input_id=[2, 3, 6265, 6672, 4073, 3249, 4034, 8732, 4292, 6722, 4076, 8553, 3, 11655, 4279, 8553, 18, 6336, 4481, 22014, 6771, 4204, 4112, 8538, 4147, 27233, 35, 18790, 4086, 24, 4469, 10749, 14043, 4006, 4073, 4325, 3311, 4112, 6392, 4110, 2734, 4219, 3249, 4576, 6216, 18, 3, 3311, 4116, 4150, 7149, 18790, 4112, 2633, 4151, 4076, 5240, 4050, 6698, 4467, 4029, 4070, 13177, 4479, 4065, 4150, 35, 3, 6698, 4467, 4029, 4034, 9908, 26885, 11684, 25845, 4204, 10561, 18, 2373, 6289, 4279, 4147, 2054, 3249, 4154, 4161, 10397, 35, 3, 2279, 13090, 4192, 2024, 4112, 6249, 4234, 15532, 4403, 4292, 2010, 4219, 4451, 4112, 4244, 4150, 11431, 4221, 4007, 3249, 16868, 4479, 4150, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6336, 4481, 22014, 6771, 4204, 3], [8732, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [93, 6756, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [15532, 4403, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='snowy-hat-8324:관광_식당_11-3', input_id=[2, 3, 6265, 6672, 4073, 3249, 4034, 8732, 4292, 6722, 4076, 8553, 3, 11655, 4279, 8553, 18, 6336, 4481, 22014, 6771, 4204, 4112, 8538, 4147, 27233, 35, 18790, 4086, 24, 4469, 10749, 14043, 4006, 4073, 4325, 3311, 4112, 6392, 4110, 2734, 4219, 3249, 4576, 6216, 18, 3, 3311, 4116, 4150, 7149, 18790, 4112, 2633, 4151, 4076, 5240, 4050, 6698, 4467, 4029, 4070, 13177, 4479, 4065, 4150, 35, 3, 6698, 4467, 4029, 4034, 9908, 26885, 11684, 25845, 4204, 10561, 18, 2373, 6289, 4279, 4147, 2054, 3249, 4154, 4161, 10397, 35, 3, 2279, 13090, 4192, 2024, 4112, 6249, 4234, 15532, 4403, 4292, 2010, 4219, 4451, 4112, 4244, 4150, 11431, 4221, 4007, 3249, 16868, 4479, 4150, 3, 6243, 4279, 4219, 12154, 27672, 4070, 3249, 4154, 4147, 27233, 35, 3, 3234, 18, 18, 8784, 4283, 27672, 4073, 3249, 4065, 4150, 35, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6336, 4481, 22014, 6771, 4204, 3], [8732, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [8784, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [93, 6756, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [15532, 4403, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='snowy-hat-8324:관광_식당_11-4', input_id=[2, 3, 6265, 6672, 4073, 3249, 4034, 8732, 4292, 6722, 4076, 8553, 3, 11655, 4279, 8553, 18, 6336, 4481, 22014, 6771, 4204, 4112, 8538, 4147, 27233, 35, 18790, 4086, 24, 4469, 10749, 14043, 4006, 4073, 4325, 3311, 4112, 6392, 4110, 2734, 4219, 3249, 4576, 6216, 18, 3, 3311, 4116, 4150, 7149, 18790, 4112, 2633, 4151, 4076, 5240, 4050, 6698, 4467, 4029, 4070, 13177, 4479, 4065, 4150, 35, 3, 6698, 4467, 4029, 4034, 9908, 26885, 11684, 25845, 4204, 10561, 18, 2373, 6289, 4279, 4147, 2054, 3249, 4154, 4161, 10397, 35, 3, 2279, 13090, 4192, 2024, 4112, 6249, 4234, 15532, 4403, 4292, 2010, 4219, 4451, 4112, 4244, 4150, 11431, 4221, 4007, 3249, 16868, 4479, 4150, 3, 6243, 4279, 4219, 12154, 27672, 4070, 3249, 4154, 4147, 27233, 35, 3, 3234, 18, 18, 8784, 4283, 27672, 4073, 3249, 4065, 4150, 35, 3, 11946, 4279, 17164, 8784, 4283, 27672, 4073, 4034, 3123, 4154, 4114, 4116, 4150, 18, 3, 7258, 10238, 27672, 4239, 6304, 6722, 4076, 8553, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6336, 4481, 22014, 6771, 4204, 3], [8732, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [10238, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [93, 6756, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [15532, 4403, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='snowy-hat-8324:관광_식당_11-5', input_id=[2, 3, 6265, 6672, 4073, 3249, 4034, 8732, 4292, 6722, 4076, 8553, 3, 11655, 4279, 8553, 18, 6336, 4481, 22014, 6771, 4204, 4112, 8538, 4147, 27233, 35, 18790, 4086, 24, 4469, 10749, 14043, 4006, 4073, 4325, 3311, 4112, 6392, 4110, 2734, 4219, 3249, 4576, 6216, 18, 3, 3311, 4116, 4150, 7149, 18790, 4112, 2633, 4151, 4076, 5240, 4050, 6698, 4467, 4029, 4070, 13177, 4479, 4065, 4150, 35, 3, 6698, 4467, 4029, 4034, 9908, 26885, 11684, 25845, 4204, 10561, 18, 2373, 6289, 4279, 4147, 2054, 3249, 4154, 4161, 10397, 35, 3, 2279, 13090, 4192, 2024, 4112, 6249, 4234, 15532, 4403, 4292, 2010, 4219, 4451, 4112, 4244, 4150, 11431, 4221, 4007, 3249, 16868, 4479, 4150, 3, 6243, 4279, 4219, 12154, 27672, 4070, 3249, 4154, 4147, 27233, 35, 3, 3234, 18, 18, 8784, 4283, 27672, 4073, 3249, 4065, 4150, 35, 3, 11946, 4279, 17164, 8784, 4283, 27672, 4073, 4034, 3123, 4154, 4114, 4116, 4150, 18, 3, 7258, 10238, 27672, 4239, 6304, 6722, 4076, 8553, 3, 19868, 4234, 4641, 4557, 4112, 8538, 4147, 27233, 35, 6270, 8297, 4034, 8830, 4083, 10561, 18, 3, 3311, 4576, 20959, 9826, 6363, 30, 9284, 4073, 21, 4282, 8866, 4070, 4521, 4283, 27233, 35, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 4, 4, 4, 4, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6336, 4481, 22014, 6771, 4204, 3], [8732, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [10238, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [93, 6756, 3, 0, 0, 0], [21, 3, 0, 0, 0, 0], [6363, 30, 9284, 3, 0, 0], [9826, 3, 0, 0, 0, 0], [19868, 4234, 4641, 4557, 3, 0], [21832, 11764, 3, 0, 0, 0], [15532, 4403, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='snowy-hat-8324:관광_식당_11-6', input_id=[2, 3, 6265, 6672, 4073, 3249, 4034, 8732, 4292, 6722, 4076, 8553, 3, 11655, 4279, 8553, 18, 6336, 4481, 22014, 6771, 4204, 4112, 8538, 4147, 27233, 35, 18790, 4086, 24, 4469, 10749, 14043, 4006, 4073, 4325, 3311, 4112, 6392, 4110, 2734, 4219, 3249, 4576, 6216, 18, 3, 3311, 4116, 4150, 7149, 18790, 4112, 2633, 4151, 4076, 5240, 4050, 6698, 4467, 4029, 4070, 13177, 4479, 4065, 4150, 35, 3, 6698, 4467, 4029, 4034, 9908, 26885, 11684, 25845, 4204, 10561, 18, 2373, 6289, 4279, 4147, 2054, 3249, 4154, 4161, 10397, 35, 3, 2279, 13090, 4192, 2024, 4112, 6249, 4234, 15532, 4403, 4292, 2010, 4219, 4451, 4112, 4244, 4150, 11431, 4221, 4007, 3249, 16868, 4479, 4150, 3, 6243, 4279, 4219, 12154, 27672, 4070, 3249, 4154, 4147, 27233, 35, 3, 3234, 18, 18, 8784, 4283, 27672, 4073, 3249, 4065, 4150, 35, 3, 11946, 4279, 17164, 8784, 4283, 27672, 4073, 4034, 3123, 4154, 4114, 4116, 4150, 18, 3, 7258, 10238, 27672, 4239, 6304, 6722, 4076, 8553, 3, 19868, 4234, 4641, 4557, 4112, 8538, 4147, 27233, 35, 6270, 8297, 4034, 8830, 4083, 10561, 18, 3, 3311, 4576, 20959, 9826, 6363, 30, 9284, 4073, 21, 4282, 8866, 4070, 4521, 4283, 27233, 35, 3, 6259, 17788, 18, 8866, 4086, 4192, 4023, 4829, 10397, 35, 3, 2287, 7149, 10472, 4034, 6477, 11560, 4150, 35, 8117, 4034, 6259, 4283, 27233, 35, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 4, 4, 4, 4, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6336, 4481, 22014, 6771, 4204, 3], [8732, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [10238, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [93, 6756, 3, 0, 0, 0], [21, 3, 0, 0, 0, 0], [6363, 30, 9284, 3, 0, 0], [9826, 3, 0, 0, 0, 0], [19868, 4234, 4641, 4557, 3, 0], [21832, 11764, 3, 0, 0, 0], [15532, 4403, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='snowy-hat-8324:관광_식당_11-7', input_id=[2, 3, 6265, 6672, 4073, 3249, 4034, 8732, 4292, 6722, 4076, 8553, 3, 11655, 4279, 8553, 18, 6336, 4481, 22014, 6771, 4204, 4112, 8538, 4147, 27233, 35, 18790, 4086, 24, 4469, 10749, 14043, 4006, 4073, 4325, 3311, 4112, 6392, 4110, 2734, 4219, 3249, 4576, 6216, 18, 3, 3311, 4116, 4150, 7149, 18790, 4112, 2633, 4151, 4076, 5240, 4050, 6698, 4467, 4029, 4070, 13177, 4479, 4065, 4150, 35, 3, 6698, 4467, 4029, 4034, 9908, 26885, 11684, 25845, 4204, 10561, 18, 2373, 6289, 4279, 4147, 2054, 3249, 4154, 4161, 10397, 35, 3, 2279, 13090, 4192, 2024, 4112, 6249, 4234, 15532, 4403, 4292, 2010, 4219, 4451, 4112, 4244, 4150, 11431, 4221, 4007, 3249, 16868, 4479, 4150, 3, 6243, 4279, 4219, 12154, 27672, 4070, 3249, 4154, 4147, 27233, 35, 3, 3234, 18, 18, 8784, 4283, 27672, 4073, 3249, 4065, 4150, 35, 3, 11946, 4279, 17164, 8784, 4283, 27672, 4073, 4034, 3123, 4154, 4114, 4116, 4150, 18, 3, 7258, 10238, 27672, 4239, 6304, 6722, 4076, 8553, 3, 19868, 4234, 4641, 4557, 4112, 8538, 4147, 27233, 35, 6270, 8297, 4034, 8830, 4083, 10561, 18, 3, 3311, 4576, 20959, 9826, 6363, 30, 9284, 4073, 21, 4282, 8866, 4070, 4521, 4283, 27233, 35, 3, 6259, 17788, 18, 8866, 4086, 4192, 4023, 4829, 10397, 35, 3, 2287, 7149, 10472, 4034, 6477, 11560, 4150, 35, 8117, 4034, 6259, 4283, 27233, 35, 3, 10472, 4034, 6477, 4279, 4219, 3249, 4219, 8117, 4086, 6259, 17788, 18, 2373, 8372, 4279, 4147, 3283, 3249, 4154, 4147, 27233, 35, 3, 6231, 4297, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 4, 4, 4, 4, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6336, 4481, 22014, 6771, 4204, 3], [8732, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [10238, 3, 0, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [93, 6756, 3, 0, 0, 0], [21, 3, 0, 0, 0, 0], [6363, 30, 9284, 3, 0, 0], [9826, 3, 0, 0, 0, 0], [19868, 4234, 4641, 4557, 3, 0], [21832, 11764, 3, 0, 0, 0], [15532, 4403, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [6265, 6672, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0], [21832, 11764, 3, 0, 0, 0]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='polished-poetry-0057:관광_9-0', input_id=[2, 3, 7596, 4292, 3755, 4228, 18781, 6265, 10806, 4073, 3249, 11649, 4150, 35, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [7596, 3, 0], [21832, 11764, 3], [6265, 10806, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='polished-poetry-0057:관광_9-1', input_id=[2, 3, 7596, 4292, 3755, 4228, 18781, 6265, 10806, 4073, 3249, 11649, 4150, 35, 3, 6265, 10806, 4073, 7596, 4007, 6259, 4283, 2084, 4007, 24874, 28060, 16301, 15550, 12178, 4007, 3249, 4576, 6216, 18, 3, 3158, 2279, 7149, 9068, 3305, 6449, 4076, 8553, 18, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [28060, 16301, 15550, 12178, 3], [7596, 3, 0, 0, 0], [21832, 11764, 3, 0, 0], [6265, 10806, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='polished-poetry-0057:관광_9-2', input_id=[2, 3, 7596, 4292, 3755, 4228, 18781, 6265, 10806, 4073, 3249, 11649, 4150, 35, 3, 6265, 10806, 4073, 7596, 4007, 6259, 4283, 2084, 4007, 24874, 28060, 16301, 15550, 12178, 4007, 3249, 4576, 6216, 18, 3, 3158, 2279, 7149, 9068, 3305, 6449, 4076, 8553, 18, 3, 28060, 16301, 15550, 12178, 4234, 9068, 4034, 6265, 27439, 10732, 11684, 4096, 10561, 18, 3, 6449, 4076, 4114, 4034, 4396, 4073, 20025, 4294, 18790, 4086, 3305, 6449, 4076, 8553, 18, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [28060, 16301, 15550, 12178, 3], [7596, 3, 0, 0, 0], [21832, 11764, 3, 0, 0], [6265, 10806, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0]], slot_positions=None, domain_id=None),\n",
       " OpenVocabDSTFeature(guid='polished-poetry-0057:관광_9-3', input_id=[2, 3, 7596, 4292, 3755, 4228, 18781, 6265, 10806, 4073, 3249, 11649, 4150, 35, 3, 6265, 10806, 4073, 7596, 4007, 6259, 4283, 2084, 4007, 24874, 28060, 16301, 15550, 12178, 4007, 3249, 4576, 6216, 18, 3, 3158, 2279, 7149, 9068, 3305, 6449, 4076, 8553, 18, 3, 28060, 16301, 15550, 12178, 4234, 9068, 4034, 6265, 27439, 10732, 11684, 4096, 10561, 18, 3, 6449, 4076, 4114, 4034, 4396, 4073, 20025, 4294, 18790, 4086, 3305, 6449, 4076, 8553, 18, 3, 7258, 18, 20025, 4034, 9141, 26808, 6218, 27738, 26817, 4007, 4219, 18790, 4112, 24, 4469, 10561, 18, 3, 3170, 6851, 17788, 18, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [28060, 16301, 15550, 12178, 3], [7596, 3, 0, 0, 0], [21832, 11764, 3, 0, 0], [6265, 10806, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0]], slot_positions=None, domain_id=None)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "==> input ids\n",
    "    턴과 턴 사이에는 [SEP] 토큰\n",
    "    tokenizer.encode\n",
    "    max_length 길이 맞추고 마지막에 [CLS] {input_ids} [SEP] 로 만듬\n",
    "'''\n",
    "#train_dataset = train_features\n",
    "train_features = []\n",
    "for example in tqdm(train_example):\n",
    "    # 턴과 턴 사이에 [SEP]\n",
    "    dialogue_context = \" [SEP] \".join(example.context_turns + example.current_turn)\n",
    "    # tokenizer.encode\n",
    "    input_ids = tokenizer.encode(dialogue_context, add_special_tokens=False)\n",
    "    # max_length 길이 맞추고 마지막에 [CLS] {input_ids} [SEP] 로 만듬\n",
    "    max_seq_length = 512 - 2 #마지막에 [CLS], [SEP] 붙일 것을 빼줌\n",
    "\n",
    "    # 만약 max_seq_length 보다 input_ids가 크면 왼쪽부터 truncate. 왼쪽이 오래된 발화이기 때문이다.\n",
    "    if len(input_ids) > max_seq_length:\n",
    "        gap_idx = len(input_ids) - max_seq_length\n",
    "        input_ids = input_ids[gap_idx:]\n",
    "    # 앞 뒤에 토큰 붙여주기\n",
    "    input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id]\n",
    "    '''\n",
    "    ==> segment ids\n",
    "    bert tokenizer를 사용할 때, 문장을 구분해주고자 할 때 쓰는 id 이다.\n",
    "    '''\n",
    "    firt_sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "    segment_id = [0] * len(input_ids[: firt_sep_idx + 1]) + [1] * len(\n",
    "        input_ids[firt_sep_idx + 1 :]\n",
    "    )\n",
    "    '''\n",
    "    ==> target id\n",
    "    example의 label(domain-slot-value)을 dict(\"domain-slot\":\"value\")로 변환\n",
    "    slot meta 전체 slot에 대하여 해당 state의 value 값을 업데이트, 없으면 0\n",
    "    tokenizer.encode\n",
    "    뒤에 [SEP] 토큰 부착\n",
    "    길이를 같게 만들어 주기 위해서 max_length 만큼 [PAD] 토큰 부착\n",
    "    '''\n",
    "    state = convert_state_dict(example.label)\n",
    "    gate_ids = []\n",
    "    target_ids = []\n",
    "    for slot in slot_meta:\n",
    "        #slot meta의 slot에서 현재 state의 값이 있으면 해당 슬롯의 value를 가져오고 없으면 none\n",
    "        value = state.get(slot, \"none\")\n",
    "        #tokenizer.encode\n",
    "        target_id = tokenizer.encode(value, add_special_tokens=False)\n",
    "        target_id += [tokenizer.sep_token_id]\n",
    "        '''\n",
    "        ==> gate id\n",
    "        target id의 value를 gating2id를 통해 index로 변환\n",
    "        '''\n",
    "        gating2id = {\"none\":0, \"dontcare\":1, \"yes\":2, \"no\":3, \"ptr\":4}\n",
    "        #전체 gate_ids, target_ids에 추가\n",
    "        gate_ids.append(gating2id.get(value, gating2id[\"ptr\"]))\n",
    "        target_ids.append(target_id)\n",
    "    # max length 만큼 [PAD] 부착\n",
    "    max_length = max(list(map(len, target_ids)))\n",
    "    target_ids = [target_id + [tokenizer.pad_token_id]*(max_length-len(target_id)) for target_id in target_ids]\n",
    "    \n",
    "    #OpenVocabDSTFeature(guid, input_ids, segment_ids, gating_ids, target_ids) 총 5개의 데이터 구성\n",
    "    train_features.append(OpenVocabDSTFeature(example.guid, input_ids, segment_ids, gate_ids, target_ids))\n",
    "print(f\"총 {len(train_features)} 개의 features가 있습니다\")\n",
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa4749-f1e7-4e67-a5cb-ede9e124ed6a",
   "metadata": {},
   "source": [
    "### 2. TRADE Model Architecture\n",
    "TRADE 모듈은 총 3가지로 구성되어 있다.\n",
    "- Utterance encoder - Bidirection GRU based encoder(Bert 가능)\n",
    "- Slot(State) generator - GRU based decoder(transformer decoder 가능)\n",
    "- Slot gate - pointer generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee1746b1-d3d2-43ec-86a1-e9242923d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utterance encoder\n",
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layer, dropout, proj_dim=None, pad_idx=0):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        if proj_dim:\n",
    "            self.proj_layer = nn.Linear(d_model, proj_dim, bias=False)\n",
    "        else:\n",
    "            self.proj_layer = None\n",
    "\n",
    "        self.d_model = proj_dim if proj_dim else d_model\n",
    "        self.gru = nn.GRU(\n",
    "            self.d_model,\n",
    "            self.d_model,\n",
    "            n_layer,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        mask = input_ids.eq(self.pad_idx).unsqueeze(-1)\n",
    "        x = self.embed(input_ids)\n",
    "        if self.proj_layer:\n",
    "            x = self.proj_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        o, h = self.gru(x)\n",
    "        o = o.masked_fill(mask, 0.0)\n",
    "        # bidirectional 이라 두개 이어주는거\n",
    "        output = o[:, :, : self.d_model] + o[:, :, self.d_model :]\n",
    "        hidden = h[0] + h[1]  # n_layer 고려\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3cdcc3c0-3e87-45ab-a865-8d545c081035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, hidden_size, dropout, n_gate, proj_dim=None, pad_idx=0\n",
    "    ):\n",
    "        super(SlotGenerator, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, hidden_size, padding_idx=pad_idx\n",
    "        )  # shared with encoder\n",
    "\n",
    "        if proj_dim:\n",
    "            self.proj_layer = nn.Linear(hidden_size, proj_dim, bias=False)\n",
    "        else:\n",
    "            self.proj_layer = None\n",
    "        self.hidden_size = proj_dim if proj_dim else hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            self.hidden_size, self.hidden_size, 1, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.n_gate = n_gate\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_gen = nn.Linear(self.hidden_size * 3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.w_gate = nn.Linear(self.hidden_size, n_gate)\n",
    "\n",
    "    def set_slot_idx(self, slot_vocab_idx):\n",
    "        whole = []\n",
    "        max_length = max(map(len, slot_vocab_idx))\n",
    "        for idx in slot_vocab_idx:\n",
    "            if len(idx) < max_length:\n",
    "                gap = max_length - len(idx)\n",
    "                idx.extend([self.pad_idx] * gap)\n",
    "            whole.append(idx)\n",
    "        self.slot_embed_idx = whole  # torch.LongTensor(whole)\n",
    "\n",
    "    def embedding(self, x):\n",
    "        x = self.embed(x)\n",
    "        if self.proj_layer:\n",
    "            x = self.proj_layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, encoder_output, hidden, input_masks, max_len, teacher=None\n",
    "    ):\n",
    "        print(input_masks.shape)\n",
    "        input_masks = input_masks.ne(1) #input mask 반전\n",
    "        print(input_masks.shape) # \n",
    "        # J, slot_meta : key : [domain, slot] ex> LongTensor([1,2])\n",
    "        # J,2\n",
    "        batch_size = encoder_output.size(0)\n",
    "        slot = torch.tensor(\n",
    "            self.slot_embed_idx, device=input_ids.device, dtype=torch.int64\n",
    "        )  ##\n",
    "        slot_e = torch.sum(self.embedding(slot), 1)  # J,d\n",
    "        J = slot_e.size(0)\n",
    "\n",
    "        all_point_outputs = torch.zeros(\n",
    "            batch_size, J, max_len, self.vocab_size, device=input_ids.device\n",
    "        )\n",
    "\n",
    "        # Parallel Decoding\n",
    "        w = slot_e.repeat(batch_size, 1).unsqueeze(1)\n",
    "        hidden = hidden.repeat_interleave(J, dim=1)\n",
    "        encoder_output = encoder_output.repeat_interleave(J, dim=0)\n",
    "        input_ids = input_ids.repeat_interleave(J, dim=0)\n",
    "        input_masks = input_masks.repeat_interleave(J, dim=0)\n",
    "        for k in range(max_len):\n",
    "            w = self.dropout(w)\n",
    "            _, hidden = self.gru(w, hidden)  # 1,B,D\n",
    "\n",
    "            # B,T,D * B,D,1 => B,T\n",
    "            attn_e = torch.bmm(encoder_output, hidden.permute(1, 2, 0))  # B,T,1\n",
    "            attn_e = attn_e.squeeze(-1).masked_fill(input_masks, -1e4)\n",
    "            attn_history = F.softmax(attn_e, -1)  # B,T\n",
    "\n",
    "            if self.proj_layer:\n",
    "                hidden_proj = torch.matmul(hidden, self.proj_layer.weight)\n",
    "            else:\n",
    "                hidden_proj = hidden\n",
    "\n",
    "            # B,D * D,V => B,V\n",
    "            attn_v = torch.matmul(\n",
    "                hidden_proj.squeeze(0), self.embed.weight.transpose(0, 1)\n",
    "            )  # B,V\n",
    "            attn_vocab = F.softmax(attn_v, -1)\n",
    "\n",
    "            # B,1,T * B,T,D => B,1,D\n",
    "            context = torch.bmm(attn_history.unsqueeze(1), encoder_output)  # B,1,D\n",
    "            p_gen = self.sigmoid(\n",
    "                self.w_gen(torch.cat([w, hidden.transpose(0, 1), context], -1))\n",
    "            )  # B,1\n",
    "            p_gen = p_gen.squeeze(-1)\n",
    "\n",
    "            p_context_ptr = torch.zeros_like(attn_vocab, device=input_ids.device)\n",
    "            p_context_ptr.scatter_add_(1, input_ids, attn_history)  # copy B,V\n",
    "            p_final = p_gen * attn_vocab + (1 - p_gen) * p_context_ptr  # B,V\n",
    "            _, w_idx = p_final.max(-1)\n",
    "\n",
    "            if teacher is not None:\n",
    "                w = (\n",
    "                    self.embedding(teacher[:, :, k])\n",
    "                    # .transpose(0, 1)\n",
    "                    .reshape(batch_size * J, 1, -1)\n",
    "                )\n",
    "            else:\n",
    "                w = self.embedding(w_idx).unsqueeze(1)  # B,1,D\n",
    "            if k == 0:\n",
    "                gated_logit = self.w_gate(context.squeeze(1))  # B,3\n",
    "                all_gate_outputs = gated_logit.view(batch_size, J, self.n_gate)\n",
    "            all_point_outputs[:, :, k, :] = p_final.view(batch_size, J, self.vocab_size)\n",
    "\n",
    "        return all_point_outputs, all_gate_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "330c9ee0-8cfc-4ff7-b013-da044caf67d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J X emb_dim : torch.Size([45, 4])\n",
      "J X hidden_dim : torch.Size([45, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "hidden_dim = 768\n",
    "n_layer = 1\n",
    "dropout = 0.1\n",
    "n_gate = 5\n",
    "max_len = 512\n",
    "#dataset 형식으로 바꾸어줌\n",
    "train_data = WOSDataset(train_features)\n",
    "train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=1,\n",
    "        collate_fn=processor.collate_fn,\n",
    "    )\n",
    "for batch in train_loader:\n",
    "    #word2vec 과 char을 쓰는 것이 아닌, torch embedding 사용\n",
    "    input_ids, segment_ids, input_masks, gating_ids, target_ids, guids = batch\n",
    "    utterance_encoder = GRUEncoder(vocab_size, hidden_dim, n_layer, dropout)\n",
    "    output, hidden = utterance_encoder(input_ids) # 1 X encoder_max_seq_len X hidden_dim (1 X 512 X 768)\n",
    "    \n",
    "    #Slot generator\n",
    "    '''\n",
    "    ==> tokenized slot meta\n",
    "    tokenizer.encode\n",
    "    '''\n",
    "    tokenized_slot_meta = []\n",
    "    for slot in slot_meta:\n",
    "        tokenized_slot_meta.append(\n",
    "            #하이푼을 제거하고 tokenizer.encdoing\n",
    "            tokenizer.encode(slot.replace(\"-\", \" \"), add_special_tokens=False)\n",
    "        )\n",
    "    slot_generator = SlotGenerator(vocab_size, hidden_dim, dropout, n_gate)\n",
    "    slot_generator.set_slot_idx(tokenized_slot_meta)\n",
    "    slot_generator.embed.weight = utterance_encoder.embed.weight\n",
    "    if slot_generator.proj_layer:\n",
    "        slot_generator.proj_layer.weight = utterance_encoder.proj_layer.weight\n",
    "    \n",
    "    slot = torch.LongTensor(slot_generator.slot_embed_idx).to(input_ids.device) # J X emb_dim\n",
    "    print(f\"J X emb_dim : {slot.shape}\")\n",
    "    slot_e = torch.sum(slot_generator.embed(slot), 1) # J X hidden_dim\n",
    "    print(f\"J X hidden_dim : {slot_e.shape}\")\n",
    "    J = slot_e.size(0)\n",
    "    #print(slot_generator(input_ids, output, hidden.unsqueeze(0), input_masks, target_ids.size(-1), None))\n",
    "    \n",
    "    #zero tensor initialize\n",
    "    batch_size = 1\n",
    "    all_point_outputs = torch.zeros(J, batch_size, max_len, vocab_size).to(input_ids.device)\n",
    "    all_gate_outputs = torch.zeros(J, batch_size, n_gate).to(input_ids.device)\n",
    "    \n",
    "    for j in range(J):\n",
    "        print(slot_e[j].shape) # hidden\n",
    "        w = slot_e[j].expand(batch_size, 1, hidden_dim) #b X 1 X hidden\n",
    "        print(w.shape)\n",
    "        slot_value = []\n",
    "        for k in range(max_len):\n",
    "            w = \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d83a03-fcac-4a7a-adc0-107b12a70a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}